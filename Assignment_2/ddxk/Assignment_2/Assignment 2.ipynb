{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task\n",
    "Implement matrix multiplication of two matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task\n",
    "Implement Hadamard product of two matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please use HW_gender data from Assignment 1. Use the weight to predict the height of a person. You can try different variants: per gender or for overall data. Please argue why did you prefer one variant over another in the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement \"loss\" function\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the algorithms in machine learning rely on minimizing or maximizing a function, which we call “objective function”. The group of functions that are minimized are called “loss functions”. A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome.\n",
    "\n",
    "There is not a single loss function that works for all kind of data. It depends on a number of factors including the presence of outliers, choice of machine learning algorithm, time efficiency of gradient descent, ease of finding the derivatives and confidence of predictions. The purpose of this blog series is to learn about different losses and how each of them can help data scientists.\n",
    "\n",
    "Loss functions can be broadly categorized into 2 types: Classification and Regression Loss.\n",
    "<img src=\"loss.png\" />\n",
    "Our area of interest in this homework is **Regression Loss** and I will talk about 2 regression loss functions only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Square Error, Quadratic loss, L2 Loss\n",
    "is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "<img src=\"mse.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mse(true, pred):\n",
    "    \"\"\"\n",
    "    true: array of true values    \n",
    "    pred: array of predicted values\n",
    "    \n",
    "    returns: mean square error loss\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum((true - pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Absolute Error, L1 Loss\n",
    "is another loss function used for regression models. MAE is the sum of absolute differences between our target and predicted variables. So it measures the average magnitude of errors in a set of predictions, without considering their directions. (If we consider directions also, that would be called Mean Bias Error (MBE), which is a sum of residuals/errors). The range is also 0 to ∞.\n",
    "<img src=\"mae.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(true, pred):\n",
    "    \"\"\"\n",
    "    true: array of true values    \n",
    "    pred: array of predicted values\n",
    "    \n",
    "    returns: mean absolute error loss\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(np.abs(true - pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE vs. MAE (L2 loss vs L1 loss)\n",
    "In short, using the squared error is easier to solve, but using the absolute error is more robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement \"fit\" function gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimized J(ϴ) by trial and error above — just trying lots of values and visually inspecting the resulting graph. There must be a better way? Queue gradient descent. Gradient Descent is a general function for minimizing a function, in this case the Mean Squared Error cost function.\n",
    "\n",
    "Gradient Descent basically just does what we were doing by hand — change the theta values, or parameters, bit by bit, until we hopefully arrived a minimum.\n",
    "\n",
    "We start by initializing theta0 and theta1 to any two values, say 0 for both, and go from there. Formally, the algorithm is as follows:\n",
    "\n",
    "<img src=\"gd.png\" />\n",
    "\n",
    "where α, alpha, is the learning rate, or how quickly we want to move towards the minimum. If α is too large, however, we can overshoot.\n",
    "\n",
    "<img src=\"gd2.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for  0.5  is  1.0833333333333333\n",
      "Cost for  1.0  is  0.08333333333333333\n",
      "Cost for  1.5  is  0.25\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# original data set\n",
    "X = [1, 2, 3]\n",
    "y = [1, 2.5, 3.5]\n",
    "\n",
    "# slope of best_fit_1 is 0.5\n",
    "# slope of best_fit_2 is 1.0\n",
    "# slope of best_fit_3 is 1.5\n",
    "\n",
    "hyps = [0.5, 1.0, 1.5] \n",
    "\n",
    "# mutiply the original X values by the theta \n",
    "# to produce hypothesis values for each X\n",
    "def multiply_matrix(mat, theta):\n",
    "    mutated = []\n",
    "    for i in range(len(mat)):\n",
    "        mutated.append(mat[i] * theta)\n",
    "\n",
    "    return mutated\n",
    "\n",
    "# calculate cost by looping each sample\n",
    "# subtract hyp(x) from y\n",
    "# square the result\n",
    "# sum them all together\n",
    "def calc_cost(m, X, y):\n",
    "    total = 0\n",
    "    for i in range(m):\n",
    "        squared_error = (y[i] - X[i]) ** 2\n",
    "        total += squared_error\n",
    "    \n",
    "    return total * (1 / (2*m))\n",
    "\n",
    "# calculate cost for each hypothesis\n",
    "for i in range(len(hyps)):\n",
    "    hyp_values = multiply_matrix(X, hyps[i])\n",
    "\n",
    "    print(\"Cost for \", hyps[i], \" is \", calc_cost(len(X), y, hyp_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement \"predict\" function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "def estimate_coef(x, y): \n",
    "    # number of observations/points \n",
    "    n = np.size(x) \n",
    "  \n",
    "    # mean of x and y vector \n",
    "    m_x, m_y = np.mean(x), np.mean(y) \n",
    "  \n",
    "    # calculating cross-deviation and deviation about x \n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "  \n",
    "    # calculating regression coefficients \n",
    "    b_1 = SS_xy / SS_xx \n",
    "    b_0 = m_y - b_1*m_x \n",
    "  \n",
    "    return(b_0, b_1) \n",
    "  \n",
    "def plot_regression_line(x, y, b): \n",
    "    # plotting the actual points as scatter plot \n",
    "    plt.scatter(x, y, color = \"m\", \n",
    "               marker = \"o\", s = 30) \n",
    "  \n",
    "    # predicted response vector \n",
    "    y_pred = b[0] + b[1]*x \n",
    "  \n",
    "    # plotting the regression line \n",
    "    plt.plot(x, y_pred, color = \"g\") \n",
    "  \n",
    "    # putting labels \n",
    "    plt.xlabel('x') \n",
    "    plt.ylabel('y') \n",
    "  \n",
    "    # function to show plot \n",
    "    plt.show() \n",
    "  \n",
    "# def main(): \n",
    "    # observations \n",
    "    x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
    "    y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12]) \n",
    "  \n",
    "    # estimating coefficients \n",
    "    b = estimate_coef(x, y) \n",
    "    print(\"Estimated coefficients:\\nb_0 = {}  \\ \n",
    "          \\nb_1 = {}\".format(b[0], b[1])) \n",
    "  \n",
    "    # plotting regression line \n",
    "    plot_regression_line(x, y, b) \n",
    "  \n",
    "# if __name__ == \"__main__\": \n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depict the plot of loss over iterations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the \"learning rate\" value, show the comparison to other values via loss plot\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the regression line you have found\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
