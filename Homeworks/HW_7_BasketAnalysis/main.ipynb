{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">1. What is Market Basket Analysis</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**  \n",
    "  \n",
    "Market Basket Analysis is one of the key techniques used by large retailers to uncover associations between items. It works by looking for combinations of items that occur together frequently in transactions. To put it another way, it allows retailers to identify relationships between the items that people buy.  \n",
    "  \n",
    "Association Rules are widely used to analyze retail basket or transaction data, and are intended to identify strong rules discovered in transaction data using measures of interestingness, based on the concept of strong rules.  \n",
    "  \n",
    "Also it is worthy to mention that rules which are used in basket analysis do not extract an individual’s preference, rather find relationships between set of elements of every distinct transaction. This is what makes them different from collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example of Association Rules**  \n",
    "  \n",
    "Assume there are 100 customers\n",
    "* 10 of them bought milk, 8 bought butter and 6 bought both of them.\n",
    "* bought milk => bought butter\n",
    "* support = P(Milk & Butter) = 6/100 = 0.06\n",
    "* confidence = support/P(Butter) = 0.06/0.08 = 0.75\n",
    "* lift = confidence/P(Milk) = 0.75/0.10 = 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">2. Absolute Support vs Relative Support</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we have 2 transacions as follows:  \n",
    "  \n",
    "T1 = {A,A,C}  \n",
    "T2 = {A,X}  \n",
    "  \n",
    "What is the support of A.\n",
    "\n",
    "* the **absolute** support of A, i.e. the absolute number of transactions which contains A, is 2\n",
    "* the **relative** support of A, i.e. the relative number of transactions which contains A, is  2 / 2=1  \n",
    "  \n",
    "  \n",
    "  \n",
    "The point of the whole support calculation is to consider only item(sets) which appear frequently enough in different\n",
    "transactions so that one can be sure that the resulting rules are based on an actual patterns and did not appear due to chance (i.e. the strange behavior of just a few customers). This is important so that one can use the rule to make predictions about the likes/dislikes of future customers. If a pattern is based only on two customers, the applicability is ... questionable.  \n",
    "  \n",
    "Another example:  \n",
    "  \n",
    "* One customer bought A a thousand times (once), because he loves A so much\n",
    "* A second customer bought it just to try it out\n",
    "* Another 1000 customers visited the store but noone else bought A\n",
    "  \n",
    "then  \n",
    "  \n",
    "* the absolute support is 2, not 1001\n",
    "* the relative support is 2 / 1000+1+1=0.0019 and not 1001 / 1000+1+1=0.9990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">3. How many Association Rules do you know</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Association Rule: Ex. {X → Y} is a representation of finding Y on the basket which has X on it\n",
    "* Itemset: Ex. {X,Y} is a representation of the list of all items which form the association rule\n",
    "* Support: Fraction of transactions containing the itemset\n",
    "* Confidence: Probability of occurrence of {Y} given {X} is present\n",
    "* Lift: Ratio of confidence to baseline probability of occurrence of {Y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules that satisfy both a minimum support threshold (min sup) and a minimum confidence threshold (min conf ) are called strong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, association rule mining can be viewed as a two-step process:\n",
    "1. Find all frequent itemsets: By definition, each of these itemsets will occur at least as frequently as a predetermined minimum support count, min sup.\n",
    "2. Generate strong association rules from the frequent itemsets: By definition, these rules must satisfy minimum support and minimum confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">4. Closed Patterns vs Max-Patterns</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the relative support of an itemset I satisfies a prespecified minimum support threshold (i.e., the absolute support of I satisfies the corresponding minimum support count threshold), then I is a **frequent itemset** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset X is **closed** in a data set D if there exists no proper super-itemset Y such that Y has the same support count as X in D. An itemset X is a **closed frequent** itemset in set D if X is both closed and frequent in D. An itemset X is a **maximal frequent** itemset (or max-itemset) in a data set D if X is frequent, and there exists no super-itemset Y such that X ⊂ Y and Y is frequent in D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 6.2** Closed and maximal frequent itemsets.  \n",
    "  \n",
    "Suppose that a transaction database has only two transactions: (a1, a2,:::, a100) and (a1, a2,:::, a50). Let the minimum support count threshold be 1. We find two **closed frequent** itemsets and their **support counts**, that is,  \n",
    "  \n",
    "C = {(a1, a2,:::, a100) : 1 ;  (a1, a2,:::, a50) : 2 }.  \n",
    "  \n",
    "There is only one **maximal frequent** itemset: M = { (a1, a2,:::, a100) : 1 }. Notice that we cannot include (a1, a2,:::, a50) as a maximal frequent itemset because it has a frequent superset, (a1, a2,:::, a100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">Apriori Algorithm: Finding Frequent Itemsets by Confined Candidate Generation</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori employs an iterative approach known as a level-wise search, where k-itemsets are used to explore .k C 1/-itemsets. First, the set of frequent 1-itemsets is found by scanning the database to accumulate the count for each item, and collecting those items that satisfy minimum support. The resulting set is denoted by L1. Next, L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found. The finding of each Lk requires one full scan of the database. To improve the efficiency of the level-wise generation of frequent itemsets, an important property called the Apriori property is used to reduce the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apriori property**: All nonempty subsets of a frequent itemset must also be frequent.  \n",
    "  \n",
    "  \n",
    "The Apriori property is based on the following observation. By definition, if an itemset I does not satisfy the minimum support threshold, then I is not frequent. If an item A is added to the itemset I, then the resulting itemset cannot occur more frequently than I. Therefore, I union A is not frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“How is the Apriori property used in the algorithm?” To understand this, let us look at how Lk−1 is used to find Lk for k ≥ 2. A two-step process is followed, consisting of **join** and **prune** actions:  \n",
    "  \n",
    "<img src=\"pics/1.png\" />  \n",
    "  \n",
    "<img src=\"pics/2.png\" />  \n",
    "  \n",
    "<img src=\"pics/3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/4.png\" />  \n",
    "  \n",
    "<img src=\"pics/5.png\" />  \n",
    "  \n",
    "<img src=\"pics/6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/8.png\" />\n",
    "<img src=\"pics/7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">5. Improving the Efficiency of Apriori</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“How can we further improve the efficiency of Apriori-based mining?” Many variations of the Apriori algorithm have been proposed that focus on improving the efficiency of the original algorithm. Several of these variations are summarized as follows:\n",
    "<img src=\"pics/9.png\" />\n",
    "**Hash-based technique** (hashing itemsets into corresponding buckets): A hash-based\n",
    "technique can be used to reduce the size of the candidate k-itemsets, Ck, for k > 1.\n",
    "For example, when scanning each transaction in the database to generate the frequent\n",
    "1-itemsets, L1, we can generate all the 2-itemsets for each transaction, hash (i.e., map)\n",
    "them into the different buckets of a hash table structure, and increase the corresponding bucket counts (Figure 6.5). A 2-itemset with a corresponding bucket count in the\n",
    "hash table that is below the support threshold cannot be frequent and thus should\n",
    "be removed from the candidate set. Such a hash-based technique may substantially\n",
    "reduce the number of candidate k-itemsets examined (especially when k D 2).  \n",
    "  \n",
    "**Transaction** reduction (reducing the number of transactions scanned in future iterations): A transaction that does not contain any frequent k-itemsets cannot contain any\n",
    "frequent (k C 1)-itemsets. Therefore, such a transaction can be marked or removed\n",
    "from further consideration because subsequent database scans for j-itemsets, where\n",
    "j > k, will not need to consider such a transaction.  \n",
    "  \n",
    "**Partitioning** (partitioning the data to find candidate itemsets): A partitioning technique can be used that requires just two database scans to mine the frequent itemsets\n",
    "(Figure 6.6). It consists of two phases. In phase I, the algorithm divides the transactions of D into n nonoverlapping partitions. If the minimum relative support\n",
    "threshold for transactions in D is min sup, then the minimum support count for a\n",
    "partition is min sup × the number of transactions in that partition. For each partition,\n",
    "all the local frequent itemsets (i.e., the itemsets frequent within the partition) are found.\n",
    "A local frequent itemset may or may not be frequent with respect to the entire\n",
    "database, D. However, any itemset that is potentially frequent with respect to D must\n",
    "occur as a frequent itemset in at least one of the partitions.8 Therefore, all local frequent\n",
    "itemsets are candidate itemsets with respect to D. The collection of frequent itemsets\n",
    "from all partitions forms the global candidate itemsets with respect to D. In phase II,\n",
    "\n",
    "<img src=\"pics/10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">6. A Pattern-Growth Approach for Mining Frequent Itemsets</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, in many cases the Apriori candidate generate-and-test method significantly reduces the size of candidate sets, leading to good performance gain. However, it\n",
    "can suffer from two nontrivial costs:\n",
    "* It may still need to generate a huge number of candidate sets. For example, if there are 10^4 frequent 1-itemsets, the Apriori algorithm will need to generate more than 10^7 candidate 2-itemsets\n",
    "* It may need to repeatedly scan the whole database and check a large set of candidates by pattern matching. It is costly to go over each transaction in the database to determine the support of the candidate itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**“Can we design a method that mines the complete set of frequent itemsets without such\n",
    "a costly candidate generation process?”** An interesting method in this attempt is called\n",
    "frequent pattern growth, or simply **FP-growth**, which adopts a divide-and-conquer\n",
    "strategy as follows. First, it compresses the database representing frequent items into a\n",
    "frequent pattern tree, or FP-tree, which retains the itemset association information. It\n",
    "then divides the compressed database into a set of conditional databases (a special kind of\n",
    "projected database), each associated with one frequent item or “pattern fragment,” and\n",
    "mines each database separately. For each “pattern fragment,” only its associated data sets\n",
    "need to be examined. Therefore, this approach may substantially reduce the size of the\n",
    "data sets to be searched, along with the “growth” of patterns being examined. You will\n",
    "see how it works in Example 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/11.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
