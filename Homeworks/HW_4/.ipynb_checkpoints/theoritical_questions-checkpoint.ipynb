{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) What is loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Loss functions in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines learn by means of a loss function. It’s a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction. In this article we will go through several loss functions and their applications in the domain of machine/deep learning.\n",
    "\n",
    "There’s no one-size-fits-all loss function to algorithms in machine learning. There are various factors involved in choosing a loss function for specific problem such as type of machine learning algorithm chosen, ease of calculating the derivatives and to some degree the percentage of outliers in the data set.\n",
    "\n",
    "Broadly, loss functions can be classified into two major categories depending upon the type of learning task we are dealing with — Regression losses and Classification losses. In classification, we are trying to predict output from set of finite categorical values i.e Given large data set of images of hand written digits, categorizing them into one of 0–9 digits. Regression, on the other hand, deals with predicting a continuous value for example given floor area, number of rooms, size of rooms, predict the price of room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error/Quadratic Loss/L2 Loss  \n",
    "\n",
    "Mathematical formulation :-\n",
    "<img src = 'pic/1.png'>\n",
    "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values are: ['0.00000000', '0.16600000', '0.33300000']\n",
      "Actial values are: ['0.00000000', '0.25400000', '0.99800000']\n",
      "ms error is: 0.14998966666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat = np.array([0.000, 0.166, 0.333])\n",
    "y_true = np.array([0.000, 0.254, 0.998])\n",
    "\n",
    "def mse(predictions, targets):\n",
    "    differences = predictions - targets\n",
    "    differences_squared = differences ** 2\n",
    "    mean_of_differences_squared = differences_squared.mean()\n",
    "    return mean_of_differences_squared\n",
    "\n",
    "print(\"Predicted values are: \" + str([\"%.8f\" % elem for elem in y_hat]))\n",
    "print(\"Actial values are: \" + str([\"%.8f\" % elem for elem in y_true]))\n",
    "mse_val = mse(y_hat, y_true)\n",
    "print(\"ms error is: \" + str(mse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error/L1 Loss\n",
    "Mathematical formulation :-\n",
    "<img src='pic/2.png'>\n",
    "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. Like MSE, this as well measures the magnitude of error without considering their direction. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values are: : ['0.00000000', '0.16600000', '0.33300000']\n",
      "Actual values are: : ['0.00000000', '0.25400000', '0.99800000']\n",
      "mae error is: 0.251\n"
     ]
    }
   ],
   "source": [
    "def mae(predictions, targets):\n",
    "    differences = predictions - targets\n",
    "    absolute_differences = np.absolute(differences)\n",
    "    mean_absolute_differences = absolute_differences.mean()\n",
    "    return mean_absolute_differences\n",
    "\n",
    "print(\"Predicted values are: : \" + str([\"%.8f\" % elem for elem in y_hat]))\n",
    "print(\"Actual values are: : \" + str([\"%.8f\" % elem for elem in y_true]))\n",
    "mae_val = mae(y_hat, y_true)\n",
    "print (\"mae error is: \" + str(mae_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Bias Error\n",
    "Mathematical formulation :-\n",
    "<img src='pic/3.png'>\n",
    "This is much less common in machine learning domain as compared to it’s counterpart. This is same as MSE with the only difference that we don’t take absolute values. Clearly there’s a need for caution as positive and negative errors could cancel each other out. Although less accurate in practice, it could determine if the model has positive bias or negative bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values are: ['0.00000000', '0.16600000', '0.33300000']\n",
      "Actial values are: ['0.00000000', '0.25400000', '0.99800000']\n",
      "mb error is: 0.14998966666666666\n"
     ]
    }
   ],
   "source": [
    "def mbe(predictions, targets):\n",
    "    differences = predictions - targets\n",
    "    mean_of_differences = differences.mean()\n",
    "    return mean_of_differences\n",
    "\n",
    "print(\"Predicted values are: \" + str([\"%.8f\" % elem for elem in y_hat]))\n",
    "print(\"Actial values are: \" + str([\"%.8f\" % elem for elem in y_true]))\n",
    "mbe_val = mbe(y_hat, y_true)\n",
    "print(\"mb error is: \" + str(mse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss/Multi class SVM Loss\n",
    "In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence hinge loss is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it’s a convex function which makes it easy to work with usual convex optimizers used in machine learning domain.\n",
    "\n",
    "Mathematical formulation :-\n",
    "<img src='pic/4.png'>  \n",
    "Consider an example where we have three training examples and three classes to predict — Dog, cat and horse. Below the values predicted by our algorithm for each of the classes :-  \n",
    "<img src='pic/5.jpg'>  \n",
    "Computing hinge losses for all 3 training examples :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st training example\n",
    "max(0, (1.49) - (-0.39) + 1) + max(0, (4.21) - (-0.39) + 1)\n",
    "max(0, 2.88) + max(0, 5.6)\n",
    "# 2.88 + 5.6\n",
    "# 8.48 (High loss as very wrong prediction)\n",
    "\n",
    "## 2nd training example\n",
    "max(0, (-4.61) - (3.28)+ 1) + max(0, (1.46) - (3.28)+ 1)\n",
    "max(0, -6.89) + max(0, -0.82)\n",
    "# 0 + 0\n",
    "# 0 (Zero loss as correct prediction)\n",
    "\n",
    "## 3rd training example\n",
    "max(0, (1.03) - (-2.27)+ 1) + max(0, (-2.37) - (-2.27)+ 1)\n",
    "max(0, 4.3) + max(0, 0.9)\n",
    "# 4.3 + 0.9\n",
    "# 5.2 (High loss as very wrong prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss/Negative Log Likelihood\n",
    "This is the most common setting for classification problems. Cross-entropy loss increases as the predicted probability diverges from the actual label.  \n",
    "\n",
    "Mathematical formulation :-\n",
    "\n",
    "<img src = 'pic/6.png'>\n",
    "\n",
    "Notice that when actual label is 1 (y(i) = 1), second half of function disappears whereas in case actual label is 0 (y(i) = 0) first half is dropped off. In short, we are just multiplying the log of the actual predicted probability for the ground truth class. An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss is: 0.7135329699138555\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.96]])\n",
    "targets = np.array([[0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-10):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5)))/N\n",
    "    return ce_loss\n",
    "\n",
    "cross_entropy_loss = cross_entropy(predictions, targets)\n",
    "print (\"Cross entropy loss is: \" + str(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Loss function vs RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS\n",
    "In a model with a single explanatory variable, RSS is given by:\n",
    "\n",
    "<img src = \"https://wikimedia.org/api/rest_v1/media/math/render/svg/2f6526aa487b4dc460792bf1eeee79b2bba77709\" >\n",
    "or  \n",
    "<img src = \"https://wikimedia.org/api/rest_v1/media/math/render/svg/63e1a994055df3be373f8f85a194e3bd1f750e3e\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we are wandering an interesting question:  \n",
    "\n",
    "Does minimizing the RSS in your model always equate to minimizing the MSE as MSE=1/n * RSS?\n",
    "\n",
    "The answer is **YES**  \n",
    "\n",
    "This is a trivial application of optimisation rules. So long as n is constant (i.e., does not depend on θ) then for any objective function F and any other function h you have:  \n",
    "\n",
    "argmin[F(θ)] = argmin[h(n)*F(θ)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Discrete Wavelet Transform (DWT) vs Discrete Fourier Transform (DFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrete Fourier transform (DFT) tells you the frequency components of a signal, averaged over the entire duration of the signal. The discrete wavelet transform (DWT) gives information about the frequency (actually, basis) components as well as being able to indicate what time these components occur at.  \n",
    "\n",
    "### Pros of DWT vs DFT  \n",
    "1) Time and frequency information  \n",
    "\n",
    "2) A lot of flexibility - there are many different types of DWT bases, whereas the DFT is just based on cos and sin of different frequencies (or equivalently, complex exponentials of different frequencies).  \n",
    "\n",
    "3) Because data are shattered into more components, it becomes much easier to filter in or filter out a given nonstationary waveform.  \n",
    "\n",
    "4) A lot of signals are found to be sparse in an appropriate DWT basis. This makes it easy to, for instance, filter noise out of a phoneme by using a simple binary mask in the DWT domain.\n",
    "### Cons of DWT vs DFT\n",
    "\n",
    "1) Greater complexity. Greater complexity translates in this case into more resources required to perform the computation - more memory and/or processor cycles and/or time. If you don’t need to locate events in time, or if the signal is stationary in the frequency domain, there is no advantage to DWT, and more effort to compute the values.  \n",
    "\n",
    "2) The theory is more difficult to understand. DFTs are much easier to understand, and are more intuitive. However, understanding enough to use a library function for DWTs is not much more difficult than learning to use a DFT routine, perhaps 2 or 3 times harder.  \n",
    "\n",
    "3) The flexibility of DWTs is a two-edged sword - it is sometimes very difficult to chose which basis to use. Do you need a differentiable basis? Then you won’t want a Haar or low order Daubechies basis.  \n",
    "\n",
    "4) It is more difficult to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) How can this technique be useful for data reduction if the wavelet transformed data are of the same length as the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrete wavelet transform (DWT) is a linear signal processing technique. It transforms a\n",
    "vector into a numerically different vector (D to D’) of wavelet coefficients. The two vectors are of the\n",
    "same length. However it is useful for compression in the sense that wavelet-transformed data can be\n",
    "truncated. A small compressed approximation of the data can be retained by storing only a small\n",
    "fraction of the strongest wavelet coefficient e.g., retain all wavelet coefficients larger than some\n",
    "particular threshold and the remaining coefficients are set to zero. The resulting data representation is\n",
    "sparse. Computations that can take advantage of sparsity are very fat if performed in wavelet space.\n",
    "Given a set of coefficients, an approximation of the original data con be got by applying the inverse\n",
    "DWT. The DWT is closely related to the discrete Fourier transform (DFT) a signal processing technique\n",
    "involving sine’s and cosines. The general procedure for applying a discrete wavelet transform uses a\n",
    "hierarchical pyramid algorithm that halves the data in each iteration, resulting in fast computational\n",
    "speed. The method is as follows:   \n",
    "\n",
    "1) The length, L , of the input data vector must and integer power of 2.This condition can be met by padding the data vector with zeros as necessary.  \n",
    "\n",
    "2) Each transform involves applying two functions. The first applies some data smoothing, such as sum or weighted average .The second performs a weighted difference, which acts to bring out the detailed features of the data.\n",
    "\n",
    "3) The two functions are applied to pairs of input data, resulting in two sets of data of length L/2. In general these represent a smoothed or low frequency version so he input data and the high frequency content of it.\n",
    "\n",
    "4) The two functions are recursively applied to sets of data obtained in the previous loop, until the resulting data sets obtained are of length 2.\n",
    "\n",
    "5) A selection of values from the data sets obtained in the above iterations are designated the wavelet coefficients of the transformed data. \n",
    "\n",
    "Wavelet transforms can be applied to multidimensional data such as data cubes. Wavelet transforms\n",
    "have many real world applications, including the compression of fingerprint images, computer vision,\n",
    "and analysis of time-series data and data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) What is Model Building\n",
    "In regression analysis, model building is the process of developing a probabilistic model that best describes the relationship between the dependent and independent variables. The major issues are finding the proper form (linear or curvilinear) of the relationship and selecting which independent variables to include. In building models it is often desirable to use qualitative as well as quantitative variables.\n",
    "\n",
    "As noted above, quantitative variables measure how much or how many; qualitative variables represent types or categories. For instance, suppose it is of interest to predict sales of an iced tea that is available in either bottles or cans. Clearly, the independent variable “container type” could influence the dependent variable “sales.” Container type is a qualitative variable, however, and must be assigned numerical values if it is to be used in a regression study. So-called dummy variables are used to represent qualitative variables in regression analysis. For example, the dummy variable x could be used to represent container type by setting x = 0 if the iced tea is packaged in a bottle and x = 1 if the iced tea is in a can. If the beverage could be placed in glass bottles, plastic bottles, or cans, it would require two dummy variables to properly represent the qualitative variable container type. In general, k - 1 dummy variables are needed to model the effect of a qualitative variable that may assume k values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) What is Heuristic Method\n",
    "There are 2d possible attribute combinations of d attributes\n",
    "\n",
    "Typical heuristic attribute selection methods:\n",
    "\n",
    "1) Best step-wise feature selection:\n",
    "\n",
    "        The best single-attribute is picked first\n",
    "\n",
    "        Then next best attribute condition to the first, ...\n",
    "\n",
    "2) Step-wise attribute elimination:\n",
    "\n",
    "        Repeatedly eliminate the worst attribute\n",
    "\n",
    "3) Best combined attribute selection and elimination\n",
    "\n",
    "    At each step, the procedure selects the best attribute and removes the worst from among the remaining attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) What is Greedy Search\n",
    "A greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage[1] with the intent of finding a global optimum. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.\n",
    "\n",
    "For example, a greedy strategy for the traveling salesman problem (which is of a high computational complexity) is the following heuristic: \"At each step of the journey, visit the nearest unvisited city.\" This heuristic does not intend to find a best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids, and give constant-factor approximations to optimization problems with submodular structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) What is Stratified Sampling\n",
    "This is also called a cluster sampling.\n",
    "\n",
    "Partition the data set, and draw samples from each\n",
    "partition (proportionally, i.e., approximately the same\n",
    "percentage of the data)\n",
    "\n",
    "Used in conjunction with skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Simple random sampling vs Sampling without replacement vs Sampling with replacement\n",
    "1) Simple random sampling\n",
    "       \n",
    "       There is an equal probability of selecting any particular item\n",
    "       \n",
    "2) Sampling without replacement\n",
    "\n",
    "        Once an object is selected, it is removed from the population\n",
    "        \n",
    "3) Sampling with replacement\n",
    "\n",
    "        A selected object is not removed from the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
